diff --git a/models/lotka-volterra/f_known1.pt b/models/lotka-volterra/f_known1.pt
index e746e94..76b2769 100644
Binary files a/models/lotka-volterra/f_known1.pt and b/models/lotka-volterra/f_known1.pt differ
diff --git a/models/lotka-volterra/f_unknown1.pt b/models/lotka-volterra/f_unknown1.pt
index 3ab5a3b..d2f118a 100644
Binary files a/models/lotka-volterra/f_unknown1.pt and b/models/lotka-volterra/f_unknown1.pt differ
diff --git a/scripts/lotka-volterra-upinn.py b/scripts/lotka-volterra-upinn.py
index 2a69560..dce54d2 100644
--- a/scripts/lotka-volterra-upinn.py
+++ b/scripts/lotka-volterra-upinn.py
@@ -1,6 +1,7 @@
 # Torch
 import torch
 import torch.nn as nn
+import torch.nn.init as init
 
 # Custom imports
 import os
@@ -25,31 +26,48 @@ import wandb
 # # Parameters
 # alpha, beta, gamma, delta = 2/3, 4/3, 1.0, 1.0
 # x0, y0 = 1.0, 1.0
-alpha, beta, gamma, delta = 1.3, 0.9, 0.8, 1.8
-x0, y0 = 0.44249296, 4.6280594
+# alpha, beta, gamma, delta = 1.3, 0.9, 0.8, 1.8
+# x0, y0 = 0.44249296, 4.6280594
+alpha, beta, gamma, delta = 2/3, 4/3, 1.0, 1.0
+x0, y0 = 1.0, 1.0
 LV = LotkaVolterra(alpha, beta, gamma, delta, torch.tensor([x0, y0], dtype=torch.float32))
 
-time_int = [0, 3]
-N = 10000
+time_int = [0, 40]
+train_test = 0.5
+N = 800
 t = torch.linspace(time_int[0], time_int[1], N)
 X = LV.solve(t)
+train_idx = torch.arange(0, train_test*N, dtype=torch.long)
+test_idx = torch.arange(train_test*N, N, dtype=torch.long)
 
 # Sample subset and add noise
-t_s, X_s = sample_with_noise(10, t, X, epsilon=5e-3)
+t_s, X_s = sample_with_noise(10, t[train_idx], X, epsilon=5e-3)
 
 # Setup neural networks
-# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
-device = torch.device('cpu')
-f_known = FNN([1, 64, 64, 2])
-f_known.to(device)
-f_unknown = FNN([2, 16, 16, 2])
-f_unknown.to(device)
+
+device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
+print("Running on:", device)
+f_known = FNN(
+    dims=[1, 16, 16, 16, 2],
+    hidden_act=nn.Tanh(),
+    output_act=nn.Softplus(),
+    weight_init=init.xavier_normal_,
+    bias_init=init.zeros_
+).to(device)
+f_unknown = FNN(
+    dims=[2, 16, 16, 16, 2],
+    hidden_act=nn.Tanh(),
+    output_act=nn.Identity(),
+    weight_init=init.xavier_normal_,
+    bias_init=init.zeros_
+).to(device)
 
 
 # Setup Weights and Biases for online logging
 wandb.init(
     project='Master-Thesis',
-    name='UPINN-Lotka-Volterra',
+    group='UPINN',
+    name='Lotka-Volterra',
     notes='Training f_known and f_unknown for 30000 epochs followed by training f_known for an additional 30000 with collacation points outside data range.',
     job_type='Train',
     save_code=True,
@@ -71,29 +89,31 @@ optimizer = torch.optim.Adam([*f_known.parameters(), *f_unknown.parameters()], l
 lambda1, lambda2, lambda3 = 1, 1, 1
 
 # Move the data to the device and convert to float
-X_f = X.to(device)
-t_f = t.to(device).unsqueeze(-1).requires_grad_(True)
+t_f = t[train_idx].to(device).unsqueeze(-1).requires_grad_(True)
 t_s = t_s.to(device).unsqueeze(-1)
 X_s = X_s.to(device)
+t0 = torch.tensor([[0.0]], device=device).float()
+X0 = LV.X0.unsqueeze(0).to(device)
 
-# Data for evaluation
-t_eval = torch.linspace(time_int[0], time_int[0]+2*(time_int[1]-time_int[0]), 1000)
-X_true = LV.solve(t_eval)
+# Setup scaling layer
+f_known.scale_fn = lambda t_: (t_-t.min())/(t.max()-t.min())
+mu, sigma = 0, 2
+epsilon = 1e-8
+f_unknown.scale_fn = lambda x: (x-mu)/(sigma+epsilon)
 
 for epoch in range(wandb.config["Epochs"]):
 
     # Stop training of f_unknown after 30000 epochs and make collacation points outside the training data
     if epoch == 30000:
         optimizer = torch.optim.Adam(f_known.parameters(), lr=wandb.config["learning_rate"])
-        t_f = torch.linspace(time_int[0], time_int[0]+2*(time_int[1]-time_int[0]), 2*N).to(device).unsqueeze(-1).requires_grad_(True)
+        t_f = t.to(device).unsqueeze(-1).requires_grad_(True)
 
     def closure():
         optimizer.zero_grad()
         
         # Initial condition loss
-        t0 = torch.tensor([[0.0]], device=device).float()
         X0_pred = f_known(t0)
-        ic_loss = nn.MSELoss()(X0_pred, LV.X0.unsqueeze(0))
+        ic_loss = nn.MSELoss()(X0_pred, X0)
 
         # Known dynamics loss
         X_f = f_known(t_f)
@@ -133,14 +153,14 @@ for epoch in range(wandb.config["Epochs"]):
         with torch.no_grad():
 
             # Evaluate the model
-            X_pred = f_known(t_eval.unsqueeze(-1))
+            X_pred = f_known(t.unsqueeze(-1).to(device))
 
             # Plot with plotly
             fig = go.Figure()
-            fig.add_scatter(x=t_eval, y=X_true[:, 0].cpu().numpy(), mode='lines', name='Prey', line=dict(dash='dash', color='green'))
-            fig.add_scatter(x=t_eval, y=X_true[:, 1].cpu().numpy(), mode='lines', name='Predator', line=dict(dash='dash', color='red'))
-            fig.add_scatter(x=t_eval, y=X_pred[:, 0], mode='lines', name='Prey (pred)', line=dict(color='green'))
-            fig.add_scatter(x=t_eval, y=X_pred[:, 1], mode='lines', name='Predator (pred)', line=dict(color='red'))
+            fig.add_scatter(x=t, y=X[:, 0].cpu().numpy(), mode='lines', name='Prey', line=dict(dash='dash', color='green'))
+            fig.add_scatter(x=t, y=X[:, 1].cpu().numpy(), mode='lines', name='Predator', line=dict(dash='dash', color='red'))
+            fig.add_scatter(x=t, y=X_pred[:, 0].cpu().numpy(), mode='lines', name='Prey (pred)', line=dict(color='green'))
+            fig.add_scatter(x=t, y=X_pred[:, 1].cpu().numpy(), mode='lines', name='Predator (pred)', line=dict(color='red'))
             # Add datapoints
             fig.add_scatter(x=t_s.squeeze().cpu().numpy(), y=X_s[:, 0].cpu().numpy(), mode='markers', name='Prey (data)', marker=dict(color='green', symbol='x'))
             fig.add_scatter(x=t_s.squeeze().cpu().numpy(), y=X_s[:, 1].cpu().numpy(), mode='markers', name='Predator (data)', marker=dict(color='red', symbol='x'))
@@ -150,17 +170,17 @@ for epoch in range(wandb.config["Epochs"]):
             wandb.log({"Solution": wandb.Plotly(fig)})
 
             # Plot missing terms
-            res = f_unknown(X_pred)
+            res = f_unknown(X_pred).cpu()
             res_dx = res[:, 0]
             res_dy = res[:, 1]
             true_res_dx = torch.zeros_like(res_dx)
-            true_res_dy = LV.gamma*X_true[:, 0]*X_true[:, 1]
+            true_res_dy = LV.gamma*X[:, 0]*X[:, 1]
 
             fig = go.Figure()
-            fig.add_scatter(x=t_eval, y=res_dx, mode='lines', name='Residual Prey', line=dict(color='green'))
-            fig.add_scatter(x=t_eval, y=res_dy, mode='lines', name='Residual Predator', line=dict(color='red'))
-            fig.add_scatter(x=t_eval, y=true_res_dx, mode='lines', name='Prey: 0', line=dict(dash='dash', color='green'))
-            fig.add_scatter(x=t_eval, y=true_res_dy, mode='lines', name='Predator: γ*x*y', line=dict(dash='dash', color='red'))
+            fig.add_scatter(x=t, y=res_dx, mode='lines', name='Residual Prey', line=dict(color='green'))
+            fig.add_scatter(x=t, y=res_dy, mode='lines', name='Residual Predator', line=dict(color='red'))
+            fig.add_scatter(x=t, y=true_res_dx, mode='lines', name='Prey: 0', line=dict(dash='dash', color='green'))
+            fig.add_scatter(x=t, y=true_res_dy, mode='lines', name='Predator: γ*x*y', line=dict(dash='dash', color='red'))
             fig.update_layout(title=f"Lotka-Volterra Missing Terms (Epoch {epoch})")
 
             # Log figure to wandb
diff --git a/test.sh b/test.sh
index 00d3f43..a31af7a 100644
--- a/test.sh
+++ b/test.sh
@@ -1,11 +1,11 @@
 #!/bin/sh
 ### General options
 ### –- specify queue --
-#BSUB -q gpuh100
+#BSUB -q gpua100
 ### -- set the job Name --
 #BSUB -J CUDA_test
 ### -- ask for number of cores (default: 1) --
-#BSUB -n 1
+#BSUB -n 4
 ### -- Select the resources: 1 gpu in exclusive process mode --
 #BSUB -gpu "num=1"
 ### -- set walltime limit: hh:mm --  maximum 24 hours for GPU-queues right now
@@ -33,7 +33,7 @@ nvidia-smi
 module load cuda
 
 # -- Activate the virtual environment --
-source /zhome/32/9/137127/Master-Thesis/.venv/bin/activate
+source /zhome/32/9/137127/.venv/bin/activate
 
 # Execute Python file
 python test.py
\ No newline at end of file
diff --git a/utils/NeuralNets.py b/utils/NeuralNets.py
index 3401bab..9c0ff45 100644
--- a/utils/NeuralNets.py
+++ b/utils/NeuralNets.py
@@ -16,25 +16,37 @@ class ScalingLayer(nn.Module):
 
 # FEEDFORWARD NEURAL NETWORK
 class FNN(nn.Module):
-    def __init__(self, dims, activation=nn.Sigmoid(), scaling=True):
+    def __init__(self, dims, hidden_act=nn.Tanh(), output_act=nn.Identity(), weight_init=None, bias_init=None, scale_fn=lambda x: x):
 
         super(FNN, self).__init__()
 
-        if scaling:
-            if type(scaling) == bool:
-                scaling = ScalingLayer()
-        self.scaling = scaling
+        # if scaling:
+        #     if type(scaling) == bool:
+        #         scaling = ScalingLayer()
+        # self.scaling = scaling
 
         self.dims = dims
         self.layers = nn.ModuleList([nn.Linear(dims[i], dims[i+1]) for i in range(len(dims)-1)])
-        self.act_fn = activation
+        self.hidden_act_fn = hidden_act
+        self.output_act_fn = output_act
+        self.scale_fn = scale_fn
+        self.inititialize_weights(weight_init, bias_init)
+
+
+    def inititialize_weights(self, weight_init, bias_init):
+        if weight_init:
+            for layer in self.layers:
+                weight_init(layer.weight)
+        if bias_init:
+            for layer in self.layers:
+                bias_init(layer.bias)
+
     
     def forward(self, x):
-        if self.scaling:
-            x = self.scaling(x)
+        x = self.scale_fn(x)
         for i, layer in enumerate(self.layers):
             x = layer(x)
             if i < len(self.layers)-1:
-                x = self.act_fn(x)
-        return x
+                x = self.hidden_act_fn(x)
+        return self.output_act_fn(x)
     
diff --git a/utils/__pycache__/DataGenerators.cpython-311.pyc b/utils/__pycache__/DataGenerators.cpython-311.pyc
index 4931582..b5f1872 100644
Binary files a/utils/__pycache__/DataGenerators.cpython-311.pyc and b/utils/__pycache__/DataGenerators.cpython-311.pyc differ
diff --git a/utils/__pycache__/NeuralNets.cpython-311.pyc b/utils/__pycache__/NeuralNets.cpython-311.pyc
index 17ad8c2..b3633ab 100644
Binary files a/utils/__pycache__/NeuralNets.cpython-311.pyc and b/utils/__pycache__/NeuralNets.cpython-311.pyc differ
diff --git a/utils/__pycache__/Utils.cpython-311.pyc b/utils/__pycache__/Utils.cpython-311.pyc
index 8fc8bb7..c3297c4 100644
Binary files a/utils/__pycache__/Utils.cpython-311.pyc and b/utils/__pycache__/Utils.cpython-311.pyc differ
