diff --git a/scripts/burgers-pinn b/scripts/burgers-pinn
deleted file mode 100644
index e69de29..0000000
diff --git a/scripts/lotka-volterra-upinn.py b/scripts/lotka-volterra-upinn.py
index 35f1c14..1550da8 100644
--- a/scripts/lotka-volterra-upinn.py
+++ b/scripts/lotka-volterra-upinn.py
@@ -2,7 +2,7 @@
 import torch
 import torch.nn as nn
 import torch.nn.init as init
-import pykan
+# import pykan
 
 # Custom imports
 import os
@@ -33,8 +33,8 @@ alpha, beta, gamma, delta = 2/3, 4/3, 1.0, 1.0
 x0, y0 = 1.0, 1.0
 LV = LotkaVolterra(alpha, beta, gamma, delta, torch.tensor([x0, y0], dtype=torch.float32))
 
-time_int = [0, 40]
-train_test = 0.5
+time_int = [0, 25]
+train_test = 0.8
 N = 800
 t = torch.linspace(time_int[0], time_int[1], N)
 X = LV.solve(t)
@@ -48,22 +48,22 @@ t_s, X_s = sample_with_noise(10, t[train_idx], X, epsilon=5e-3)
 
 device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
 print("Running on:", device)
-# f_known = FNN(
-#     dims=[1, 16, 16, 16, 2],
-#     hidden_act=nn.Tanh(),
-#     output_act=nn.Softplus(),
-#     weight_init=init.xavier_normal_,
-#     bias_init=init.zeros_
-# ).to(device)
-# f_unknown = FNN(
-#     dims=[2, 16, 16, 16, 2],
-#     hidden_act=nn.Tanh(),
-#     output_act=nn.Identity(),
-#     weight_init=init.xavier_normal_,
-#     bias_init=init.zeros_
-# ).to(device)
-f_known = pykan.KAN([1, 16, 16, 16, 2], grid=3, k=3, seed=42, device=device)
-f_unknown = pykan.KAN([2, 16, 16, 16, 2], grid=3, k=3, seed=42, device=device)
+f_known = FNN(
+    dims=[1, 16, 16, 16, 2],
+    hidden_act=nn.Tanh(),
+    output_act=nn.Softplus(),
+    weight_init=init.xavier_normal_,
+    bias_init=init.zeros_
+).to(device)
+f_unknown = FNN(
+    dims=[2, 16, 16, 16, 2],
+    hidden_act=nn.Tanh(),
+    output_act=nn.Identity(),
+    weight_init=init.xavier_normal_,
+    bias_init=init.zeros_
+).to(device)
+# f_known = pykan.KAN([1, 16, 16, 16, 2], grid=3, k=3, seed=42, device=device)
+# f_unknown = pykan.KAN([2, 16, 16, 16, 2], grid=3, k=3, seed=42, device=device)
 
 # Setup Weights and Biases for online logging
 wandb.init(
diff --git a/utils/__pycache__/NeuralNets.cpython-311.pyc b/utils/__pycache__/NeuralNets.cpython-311.pyc
index b3633ab..340f8ac 100644
Binary files a/utils/__pycache__/NeuralNets.cpython-311.pyc and b/utils/__pycache__/NeuralNets.cpython-311.pyc differ
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index c8121c1..694eb42 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20241202_142447-jmvisdw1/logs/debug-internal.log
\ No newline at end of file
+run-20241202_181650-ajyvfsyw/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index 8de6081..183127b 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20241202_142447-jmvisdw1/logs/debug.log
\ No newline at end of file
+run-20241202_181650-ajyvfsyw/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index 12a1a4d..7272d52 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20241202_142447-jmvisdw1
\ No newline at end of file
+run-20241202_181650-ajyvfsyw
\ No newline at end of file
