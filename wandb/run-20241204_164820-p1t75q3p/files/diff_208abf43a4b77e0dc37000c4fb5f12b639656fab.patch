diff --git a/models/lotka-volterra/lotka-volterra-upinn-KAN.pt b/models/lotka-volterra/lotka-volterra-upinn-KAN.pt
index e369d25..1bc0316 100644
Binary files a/models/lotka-volterra/lotka-volterra-upinn-KAN.pt and b/models/lotka-volterra/lotka-volterra-upinn-KAN.pt differ
diff --git a/scripts/lotka-volterra-upinn.py b/scripts/lotka-volterra-upinn.py
index 53f138b..0de65f8 100644
--- a/scripts/lotka-volterra-upinn.py
+++ b/scripts/lotka-volterra-upinn.py
@@ -1,8 +1,5 @@
 # Torch
 import torch
-import torch.nn as nn
-import torch.nn.init as init
-# import pykan
 
 # Custom imports
 import os
@@ -10,7 +7,7 @@ import sys
 sys.path.append('./')
 from utils.NeuralNets import FNN, KAN
 from utils.DataGenerators import LotkaVolterra
-from utils.Utils import sample_with_noise
+from utils.Utils import sample_with_noise, SoftAdapt
 
 # Plotly
 import plotly.graph_objects as go
@@ -19,17 +16,22 @@ pio.templates.default = "plotly_dark"
 
 import wandb
 
-exp_name = "lotka-volterra-upinn-KAN"
+exp_name = "lotka-volterra-upinn-FNN"
+
 
-# Generate data from Lotka-Volterra model
-#   dx/dt = alpha*x - beta*x*y
-#   dy/dt = gamma*x*y - delta*y
 
 # # Parameters
 # alpha, beta, gamma, delta = 2/3, 4/3, 1.0, 1.0
 # x0, y0 = 1.0, 1.0
 # alpha, beta, gamma, delta = 1.3, 0.9, 0.8, 1.8
 # x0, y0 = 0.44249296, 4.6280594
+
+###############################################
+### Generate data from Lotka-Volterra model ###
+###############################################
+###   dx/dt = alpha*x - beta*x*y            ###
+###   dy/dt = gamma*x*y - delta*y           ###
+###############################################
 alpha, beta, gamma, delta = 2/3, 4/3, 1.0, 1.0
 x0, y0 = 1.0, 1.0
 LV = LotkaVolterra(alpha, beta, gamma, delta, torch.tensor([x0, y0], dtype=torch.float32))
@@ -43,30 +45,42 @@ train_idx = torch.arange(0, train_test*N, dtype=torch.long)
 test_idx = torch.arange(train_test*N, N, dtype=torch.long)
 
 # Sample subset and add noise
-t_s, X_s = sample_with_noise(10, t[train_idx], X, epsilon=5e-3)
-
-# Setup neural networks
-
-device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
-print("Running on:", device)
-# f_known = FNN(
-#     dims=[1, 16, 16, 16, 2],
-#     hidden_act=nn.Tanh(),
-#     output_act=nn.Softplus(),
-#     weight_init=init.xavier_normal_,
-#     bias_init=init.zeros_
-# ).to(device)
-# f_unknown = FNN(
-#     dims=[2, 16, 16, 16, 2],
-#     hidden_act=nn.Tanh(),
-#     output_act=nn.Identity(),
-#     weight_init=init.xavier_normal_,
-#     bias_init=init.zeros_
-# ).to(device)
-f_known = KAN([1, 8, 8, 8, 2]).to(device)
-f_unknown = KAN([2, 8, 8, 8, 2]).to(device)
-
-# Setup Weights and Biases for online logging
+t_d, X_d = sample_with_noise(10, t[train_idx], X, epsilon=5e-3)
+
+# Move the data to the device and convert to float
+
+data = dict(
+    t_b=torch.tensor([[0.0]]),
+    X_b=LV.X0.unsqueeze(0),
+    t_d=t_d.unsqueeze(-1),
+    X_d=X_d,
+    t_c=t[train_idx].unsqueeze(-1).requires_grad_(True),
+)
+
+###############################################
+### Setup the neural networks for training  ###
+###############################################
+u = FNN(
+    dims=[1, 16, 16, 16, 2],
+    hidden_act=torch.nn.Tanh(),
+    output_act=torch.nn.Softplus(),
+)
+G = FNN(
+    dims=[2, 16, 16, 16, 2],
+    hidden_act=torch.nn.Tanh(),
+    output_act=torch.nn.ReLU(),
+)
+
+# Setup scaling layer
+u.scale_fn = lambda t_: (t_-t.min())/(t.max()-t.min())
+mu, sigma = 0, 2
+epsilon = 1e-8
+G.scale_fn = lambda x: (x-mu)/(sigma+epsilon)
+
+
+##############################################
+### Setup WandB for logging and monitoring ###
+##############################################
 wandb.init(
     project='Master-Thesis',
     group='UPINN',
@@ -76,120 +90,157 @@ wandb.init(
     save_code=True,
     config={
         "learning_rate": 1e-3,
-        "Archtechture": "KAN",
+        "Archtechture": "FNN",
         "Problem": "Lotka-Volterra",
-        "Epochs": 100000,
+        "Epochs": 30000,
         "Optimizer": "AdamW",
     }
 )
 table = wandb.Table(columns=["Solution"])
 
-optimizer = torch.optim.AdamW([*f_known.parameters(), *f_unknown.parameters()], lr=wandb.config["learning_rate"], weight_decay=1e-4)
-# optimizer = torch.optim.LBFGS([*f_known.parameters(), *f_unknown.parameters()], lr=1, history_size=10, line_search_fn="strong_wolfe", tolerance_grad=1e-32, tolerance_change=1e-32)
+#####################
+### Training loop ###
+#####################
+def train(
+        u: torch.nn.Module,
+        G: torch.nn.Module,
+        data: dict,
+        optimizer: torch.optim.Optimizer = torch.optim.Adam,
+        lr: float = 1e-3,
+        weight_decay: float = 0.0,
+        epochs: int = 1000,
+        loss_tol_stop: float = None,    # Stop training if loss is below this value
+        device: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'),
+        beta_softadapt: float = 0.1,
+        plotting: dict = dict(log_plot=False, plot_interval=1000),
+        save_model: dict = dict(save=False, path='models/lotka-volterra', name='LV-UPINN'),
+):
+
+    print("Beginning training...")
+    print("Running on:", device)
+
+    # Move the model to the device
+    u.to(device); G.to(device)
+
+    # Unpack data and move to device        
+    t_b = data["t_b"].to(device)                      # Boundary points
+    X_b = data["X_b"].to(device)
+
+    t_d = data["t_d"].to(device)                      # Data points
+    X_d = data["X_d"].to(device)
+
+    t_c = data["t_c"].to(device).requires_grad_(True) # Collocation points
+
+    # Initialize parameters
+    theta = torch.nn.Parameter(torch.zeros(3))
+
+    # Initialize optimizer
+    optimizer = optimizer([*u.parameters(), *G.parameters(), theta], lr=lr, weight_decay=weight_decay)
+
+    # Initialize previous losses for SoftAdapt
+    prev_losses = torch.zeros(3).to(device)
+
+    for epoch in range(epochs):
+
+        # Stop training of f_unknown after 30000 epochs and make collacation points outside the training data
+        # if epoch == 30000:
+        #     optimizer = torch.optim.Adam(f_known.parameters(), lr=lr, weight_decay=weight_decay)
+        #     t_f = t.to(device).unsqueeze(-1).requires_grad_(True)
+
+        def closure():
+            optimizer.zero_grad()
+
+            # Boundary condition loss
+            bc_loss = torch.nn.MSELoss()(u(t_b), X_b)
+
+            # Data loss
+            data_loss = torch.nn.MSELoss()(u(t_d), X_d)
+
+            # Known PDE loss
+            X_c = u(t_c)
+            x_c, y_c = X_c.T.unsqueeze(-1)
+            dxdt = torch.autograd.grad(x_c, t_c, torch.ones_like(x_c), create_graph=True)[0]
+            dydt = torch.autograd.grad(y_c, t_c, torch.ones_like(y_c), create_graph=True)[0]
+
+            res_x, res_y = G(X_c).T.unsqueeze(-1)
+            dudt = torch.hstack([
+                dxdt - theta[0] * x_c + theta[1] * x_c * y_c - res_x,
+                dydt + theta[2] * y_c - res_y
+            ])
+            pde_loss = torch.nn.MSELoss()(dudt, torch.zeros_like(dudt))
+
+
+            # Total loss
+            cur_losses = torch.stack([bc_loss, pde_loss, data_loss])
+            lambda_ = SoftAdapt(cur_losses, prev_losses, beta=beta_softadapt)   # SoftAdapt weights
+            loss = torch.dot(lambda_, cur_losses)
+
+            # Log losses to wandb
+            wandb.log({
+                "Loss": loss.item(),
+                "BC Loss": bc_loss.item(),
+                "PDE Loss": pde_loss.item(),
+                "Data Loss": data_loss.item()
+            })
+
+            # Backpropagate
+            loss.backward(retain_graph=True)
+
+            return loss, cur_losses.clone()
+
+        loss, prev_losses = optimizer.step(closure)  # Pass the closure to the optimizer
+
+
+        # Plot solution
+        if plotting["log_plots"] and epoch % plotting["plotting_interval"] == 0:
+            with torch.no_grad():
+
+                # Evaluate the model
+                X_pred = u(t.unsqueeze(-1).to(device))
+
+                # Plot with plotly
+                fig = go.Figure()
+                fig.add_scatter(x=t, y=X[:, 0].cpu().numpy(), mode='lines', name='Prey', line=dict(dash='dash', color='green'))
+                fig.add_scatter(x=t, y=X[:, 1].cpu().numpy(), mode='lines', name='Predator', line=dict(dash='dash', color='red'))
+                fig.add_scatter(x=t, y=X_pred[:, 0].cpu().numpy(), mode='lines', name='Prey (pred)', line=dict(color='green'))
+                fig.add_scatter(x=t, y=X_pred[:, 1].cpu().numpy(), mode='lines', name='Predator (pred)', line=dict(color='red'))
+                # Add datapoints
+                fig.add_scatter(x=t_d.squeeze().cpu().numpy(), y=X_d[:, 0].cpu().numpy(), mode='markers', name='Prey (data)', marker=dict(color='green', symbol='x'))
+                fig.add_scatter(x=t_d.squeeze().cpu().numpy(), y=X_d[:, 1].cpu().numpy(), mode='markers', name='Predator (data)', marker=dict(color='red', symbol='x'))
+                fig.update_layout(title=f"Lotka-Volterra Model (Epoch {epoch})")
+                
+                # Log figure to wandb
+                wandb.log({"Solution": wandb.Plotly(fig)})
+
+                # Plot missing terms
+                res = G(X_pred).cpu()
+                res_dx = res[:, 0]
+                res_dy = res[:, 1]
+                true_res_dx = torch.zeros_like(res_dx)
+                true_res_dy = LV.gamma*X[:, 0]*X[:, 1]
 
+                fig = go.Figure()
+                fig.add_scatter(x=t, y=res_dx, mode='lines', name='Residual Prey', line=dict(color='green'))
+                fig.add_scatter(x=t, y=res_dy, mode='lines', name='Residual Predator', line=dict(color='red'))
+                fig.add_scatter(x=t, y=true_res_dx, mode='lines', name='Prey: 0', line=dict(dash='dash', color='green'))
+                fig.add_scatter(x=t, y=true_res_dy, mode='lines', name='Predator: γ*x*y', line=dict(dash='dash', color='red'))
+                fig.update_layout(title=f"Lotka-Volterra Missing Terms (Epoch {epoch})")
 
-# Weight scaling for the loss function
-lambda1, lambda2, lambda3 = 1, 1, 1
-
-# Move the data to the device and convert to float
-t_f = t[train_idx].to(device).unsqueeze(-1).requires_grad_(True)
-t_s = t_s.to(device).unsqueeze(-1)
-X_s = X_s.to(device)
-t0 = torch.tensor([[0.0]], device=device).float()
-X0 = LV.X0.unsqueeze(0).to(device)
-
-# Setup scaling layer
-f_known.scale_fn = lambda t_: (t_-t.min())/(t.max()-t.min())
-mu, sigma = 0, 2
-epsilon = 1e-8
-f_unknown.scale_fn = lambda x: (x-mu)/(sigma+epsilon)
+                # Log figure to wandb
+                wandb.log({"Missing Terms": wandb.Plotly(fig)})
+        
 
-for epoch in range(wandb.config["Epochs"]):
+        if loss_tol_stop:
+            if loss < loss_tol_stop:
+                print("Loss below tolerance. Stopping training.")
+                break
+    
+    print("Training complete.")
 
-    # Stop training of f_unknown after 30000 epochs and make collacation points outside the training data
-    if epoch == 30000:
-        optimizer = torch.optim.Adam(f_known.parameters(), lr=wandb.config["learning_rate"])
-        t_f = t.to(device).unsqueeze(-1).requires_grad_(True)
+    # Save the model
+    if save_model["save"]:
+        print("Saving model...")
+        torch.save(u.state_dict(), os.path.join(save_model["path"], save_model["name"] + '_u.pth'))
+        torch.save(G.state_dict(), os.path.join(save_model["path"], save_model["name"] + '_G.pth'))
 
-    def closure():
-        optimizer.zero_grad()
-        
-        # Initial condition loss
-        X0_pred = f_known(t0)
-        ic_loss = nn.MSELoss()(X0_pred, X0)
-
-        # Known dynamics loss
-        X_f = f_known(t_f)
-        x_f, y_f = X_f[:, 0:1], X_f[:, 1:2]
-        dxdt = torch.autograd.grad(x_f, t_f, torch.ones_like(x_f), create_graph=True)[0]
-        dydt = torch.autograd.grad(y_f, t_f, torch.ones_like(y_f), create_graph=True)[0]
-
-        res_pred = f_unknown(X_f)
-        res_x, res_y = res_pred[:, 0:1], res_pred[:, 1:2]
-        dudt = torch.hstack([
-            dxdt - LV.alpha * x_f + LV.beta * x_f * y_f - res_x,
-            dydt + LV.delta * y_f - res_y
-        ])
-        pde_loss = torch.mean(dudt[:, 0] ** 2) + torch.mean(dudt[:, 1] ** 2)
-
-        # Data loss
-        X_pred = f_known(t_s)
-        data_loss = nn.MSELoss()(X_pred, X_s)
-
-        # Total loss
-        loss = lambda1 * ic_loss + lambda2 * pde_loss + lambda3 * data_loss
-        wandb.log({
-            "Loss": loss.item(),
-            "IC Loss": ic_loss.item(),
-            "PDE Loss": pde_loss.item(),
-            "Data Loss": data_loss.item()
-        })
-
-        loss.backward(retain_graph=True)
-
-        return loss
-
-    optimizer.step(closure)  # Pass the closure to the optimizer
-
-    # Plot solution
-    if epoch % 1000 == 0:
-        with torch.no_grad():
-
-            # Evaluate the model
-            X_pred = f_known(t.unsqueeze(-1).to(device))
-
-            # Plot with plotly
-            fig = go.Figure()
-            fig.add_scatter(x=t, y=X[:, 0].cpu().numpy(), mode='lines', name='Prey', line=dict(dash='dash', color='green'))
-            fig.add_scatter(x=t, y=X[:, 1].cpu().numpy(), mode='lines', name='Predator', line=dict(dash='dash', color='red'))
-            fig.add_scatter(x=t, y=X_pred[:, 0].cpu().numpy(), mode='lines', name='Prey (pred)', line=dict(color='green'))
-            fig.add_scatter(x=t, y=X_pred[:, 1].cpu().numpy(), mode='lines', name='Predator (pred)', line=dict(color='red'))
-            # Add datapoints
-            fig.add_scatter(x=t_s.squeeze().cpu().numpy(), y=X_s[:, 0].cpu().numpy(), mode='markers', name='Prey (data)', marker=dict(color='green', symbol='x'))
-            fig.add_scatter(x=t_s.squeeze().cpu().numpy(), y=X_s[:, 1].cpu().numpy(), mode='markers', name='Predator (data)', marker=dict(color='red', symbol='x'))
-            fig.update_layout(title=f"Lotka-Volterra Model (Epoch {epoch})")
-            
-            # Log figure to wandb
-            wandb.log({"Solution": wandb.Plotly(fig)})
-
-            # Plot missing terms
-            res = f_unknown(X_pred).cpu()
-            res_dx = res[:, 0]
-            res_dy = res[:, 1]
-            true_res_dx = torch.zeros_like(res_dx)
-            true_res_dy = LV.gamma*X[:, 0]*X[:, 1]
-
-            fig = go.Figure()
-            fig.add_scatter(x=t, y=res_dx, mode='lines', name='Residual Prey', line=dict(color='green'))
-            fig.add_scatter(x=t, y=res_dy, mode='lines', name='Residual Predator', line=dict(color='red'))
-            fig.add_scatter(x=t, y=true_res_dx, mode='lines', name='Prey: 0', line=dict(dash='dash', color='green'))
-            fig.add_scatter(x=t, y=true_res_dy, mode='lines', name='Predator: γ*x*y', line=dict(dash='dash', color='red'))
-            fig.update_layout(title=f"Lotka-Volterra Missing Terms (Epoch {epoch})")
-
-            # Log figure to wandb
-            wandb.log({"Missing Terms": wandb.Plotly(fig)})
-
-
-# Save the model
-torch.save(f_known.state_dict(), 'models/lotka-volterra/'+exp_name+'.pt')
-torch.save(f_unknown.state_dict(), 'models/lotka-volterra/'+exp_name+'.pt')
\ No newline at end of file
+train(u, G, data, optimizer=torch.optim.AdamW, lr=wandb.config["learning_rate"], epochs=wandb.config["Epochs"], plotting=dict(log_plots=True, plot_interval=1000), save_model=dict(save=False, path='models/lotka-volterra', name='LV-UPINN'))
\ No newline at end of file
diff --git a/utils/NeuralNets.py b/utils/NeuralNets.py
index 8f001d9..f54face 100644
--- a/utils/NeuralNets.py
+++ b/utils/NeuralNets.py
@@ -1,6 +1,7 @@
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
+from torch.nn.init import xavier_normal_, zeros_
 import math
 
 def count_parameters(model):
@@ -8,7 +9,7 @@ def count_parameters(model):
 
 # FEEDFORWARD NEURAL NETWORK
 class FNN(nn.Module):
-    def __init__(self, dims, hidden_act=nn.Tanh(), output_act=nn.Identity(), weight_init=None, bias_init=None, scale_fn=lambda x: x):
+    def __init__(self, dims, hidden_act=nn.Tanh(), output_act=nn.Identity(), weight_init=xavier_normal_, bias_init=zeros_, scale_fn=lambda x: x):
 
         super(FNN, self).__init__()
 
diff --git a/utils/Utils.py b/utils/Utils.py
index 7d24f4f..a19346d 100644
--- a/utils/Utils.py
+++ b/utils/Utils.py
@@ -23,4 +23,15 @@ def sample_with_noise(N, t, X, epsilon=5e-3):
     # Add noise to the data
     X_noise = X + epsilon * X_bar * torch.randn(*X.shape)
 
-    return t, X_noise
\ No newline at end of file
+    return t, X_noise
+
+
+def SoftAdapt(cur_losses, prev_losses, beta=0, loss_weigthed=False):
+    f = cur_losses.detach()
+    fm1 = prev_losses.detach()
+    s = f - fm1
+    if loss_weigthed:
+        return f*torch.exp(beta*s) / torch.sum(f*torch.exp(beta*s))
+    else:
+        return torch.nn.functional.softmax(beta*s, dim=0)
+
diff --git a/utils/__pycache__/NeuralNets.cpython-311.pyc b/utils/__pycache__/NeuralNets.cpython-311.pyc
index 906ca71..ee1bc0d 100644
Binary files a/utils/__pycache__/NeuralNets.cpython-311.pyc and b/utils/__pycache__/NeuralNets.cpython-311.pyc differ
diff --git a/utils/__pycache__/Utils.cpython-311.pyc b/utils/__pycache__/Utils.cpython-311.pyc
index c3297c4..07f5af5 100644
Binary files a/utils/__pycache__/Utils.cpython-311.pyc and b/utils/__pycache__/Utils.cpython-311.pyc differ
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index 825b4db..c8ccf7a 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20241202_185341-qw5tmtdq/logs/debug-internal.log
\ No newline at end of file
+run-20241204_164820-p1t75q3p/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index d7a4ef0..7d03c7e 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20241202_185341-qw5tmtdq/logs/debug.log
\ No newline at end of file
+run-20241204_164820-p1t75q3p/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index e64e002..e7cf6f1 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20241202_185341-qw5tmtdq
\ No newline at end of file
+run-20241204_164820-p1t75q3p
\ No newline at end of file
