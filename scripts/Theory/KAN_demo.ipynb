{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 Li, Ziyao\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import *\n",
    "\n",
    "class SplineLinear(nn.Linear):\n",
    "    def __init__(self, in_features: int, out_features: int, init_scale: float = 0.1, **kw) -> None:\n",
    "        self.init_scale = init_scale\n",
    "        super().__init__(in_features, out_features, bias=False, **kw)\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        nn.init.trunc_normal_(self.weight, mean=0, std=self.init_scale)\n",
    "\n",
    "class RadialBasisFunction(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        grid_min: float = -2.,\n",
    "        grid_max: float = 2.,\n",
    "        num_grids: int = 8,\n",
    "        denominator: float = None,  # larger denominators lead to smoother basis\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.grid_min = grid_min\n",
    "        self.grid_max = grid_max\n",
    "        self.num_grids = num_grids\n",
    "        grid = torch.linspace(grid_min, grid_max, num_grids)\n",
    "        self.grid = torch.nn.Parameter(grid, requires_grad=False)\n",
    "        self.denominator = denominator or (grid_max - grid_min) / (num_grids - 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.exp(-((x[..., None] - self.grid) / self.denominator) ** 2)\n",
    "\n",
    "class FastKANLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        output_dim: int,\n",
    "        grid_min: float = -2.,\n",
    "        grid_max: float = 2.,\n",
    "        num_grids: int = 8,\n",
    "        use_base_update: bool = True,\n",
    "        use_layernorm: bool = True,\n",
    "        base_activation = F.silu,\n",
    "        spline_weight_init_scale: float = 0.1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.layernorm = None\n",
    "        if use_layernorm:\n",
    "            assert input_dim > 1, \"Do not use layernorms on 1D inputs. Set `use_layernorm=False`.\"\n",
    "            self.layernorm = nn.LayerNorm(input_dim)\n",
    "        self.rbf = RadialBasisFunction(grid_min, grid_max, num_grids)\n",
    "        self.spline_linear = SplineLinear(input_dim * num_grids, output_dim, spline_weight_init_scale)\n",
    "        self.use_base_update = use_base_update\n",
    "        if use_base_update:\n",
    "            self.base_activation = base_activation\n",
    "            self.base_linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, use_layernorm=True):\n",
    "        if self.layernorm is not None and use_layernorm:\n",
    "            spline_basis = self.rbf(self.layernorm(x))\n",
    "        else:\n",
    "            spline_basis = self.rbf(x)\n",
    "        ret = self.spline_linear(spline_basis.view(*spline_basis.shape[:-2], -1))\n",
    "        if self.use_base_update:\n",
    "            base = self.base_linear(self.base_activation(x))\n",
    "            ret = ret + base\n",
    "        return ret\n",
    "\n",
    "    def plot_curve(\n",
    "        self,\n",
    "        input_index: int,\n",
    "        output_index: int,\n",
    "        num_pts: int = 1000,\n",
    "        num_extrapolate_bins: int = 2\n",
    "    ):\n",
    "        '''this function returns the learned curves in a FastKANLayer.\n",
    "        input_index: the selected index of the input, in [0, input_dim) .\n",
    "        output_index: the selected index of the output, in [0, output_dim) .\n",
    "        num_pts: num of points sampled for the curve.\n",
    "        num_extrapolate_bins (N_e): num of bins extrapolating from the given grids. The curve \n",
    "            will be calculate in the range of [grid_min - h * N_e, grid_max + h * N_e].\n",
    "        '''\n",
    "        ng = self.rbf.num_grids\n",
    "        h = self.rbf.denominator\n",
    "        assert input_index < self.input_dim\n",
    "        assert output_index < self.output_dim\n",
    "        w = self.spline_linear.weight[\n",
    "            output_index, input_index * ng : (input_index + 1) * ng\n",
    "        ]   # num_grids,\n",
    "        x = torch.linspace(\n",
    "            self.rbf.grid_min - num_extrapolate_bins * h,\n",
    "            self.rbf.grid_max + num_extrapolate_bins * h,\n",
    "            num_pts\n",
    "        )   # num_pts, num_grids\n",
    "        with torch.no_grad():\n",
    "            y = (w * self.rbf(x.to(w.dtype))).sum(-1)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "class FastKAN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers_hidden: List[int],\n",
    "        grid_min: float = -2.,\n",
    "        grid_max: float = 2.,\n",
    "        num_grids: int = 8,\n",
    "        use_base_update: bool = True,\n",
    "        base_activation = F.silu,\n",
    "        spline_weight_init_scale: float = 0.1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            FastKANLayer(\n",
    "                in_dim, out_dim,\n",
    "                grid_min=grid_min,\n",
    "                grid_max=grid_max,\n",
    "                num_grids=num_grids,\n",
    "                use_base_update=use_base_update,\n",
    "                base_activation=base_activation,\n",
    "                spline_weight_init_scale=spline_weight_init_scale,\n",
    "            ) for in_dim, out_dim in zip(layers_hidden[:-1], layers_hidden[1:])\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionWithFastKANTransform(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        q_dim: int,\n",
    "        k_dim: int,\n",
    "        v_dim: int,\n",
    "        head_dim: int,\n",
    "        num_heads: int,\n",
    "        gating: bool = True,\n",
    "    ):\n",
    "        super(AttentionWithFastKANTransform, self).__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        total_dim = head_dim * self.num_heads\n",
    "        self.gating = gating\n",
    "        self.linear_q = FastKANLayer(q_dim, total_dim)\n",
    "        self.linear_k = FastKANLayer(k_dim, total_dim)\n",
    "        self.linear_v = FastKANLayer(v_dim, total_dim)\n",
    "        self.linear_o = FastKANLayer(total_dim, q_dim)\n",
    "        self.linear_g = None\n",
    "        if self.gating:\n",
    "            self.linear_g = FastKANLayer(q_dim, total_dim)\n",
    "        # precompute the 1/sqrt(head_dim)\n",
    "        self.norm = head_dim**-0.5\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        q: torch.Tensor,\n",
    "        k: torch.Tensor,\n",
    "        v: torch.Tensor,\n",
    "        bias: torch.Tensor = None,      # additive attention bias\n",
    "    ) -> torch.Tensor:         \n",
    "\n",
    "        wq = self.linear_q(q).view(*q.shape[:-1], 1, self.num_heads, -1) * self.norm     # *q1hc\n",
    "        wk = self.linear_k(k).view(*k.shape[:-2], 1, k.shape[-2], self.num_heads, -1)    # *1khc\n",
    "        att = (wq * wk).sum(-1).softmax(-2)     # *qkh\n",
    "        del wq, wk\n",
    "        if bias is not None:\n",
    "            att = att + bias[..., None]\n",
    "\n",
    "        wv = self.linear_v(v).view(*v.shape[:-2],1, v.shape[-2], self.num_heads, -1)     # *1khc\n",
    "        o = (att[..., None] * wv).sum(-3)        # *qhc\n",
    "        del att, wv\n",
    "\n",
    "        o = o.view(*o.shape[:-2], -1)           # *q(hc)\n",
    "\n",
    "        if self.linear_g is not None:\n",
    "            # gating, use raw query input\n",
    "            g = self.linear_g(q)\n",
    "            o = torch.sigmoid(g) * o\n",
    "\n",
    "        # merge heads\n",
    "        o = self.linear_o(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration of Kologorov Arnold Network (KAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from kan import KAN, create_dataset\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data from $f(x_1, x_2, x_3, x_4) = \\exp(\\sin(x_1^2 + x_2) + \\cos(x_3 + x_4^2))$ and train a KAN to approximate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobol = torch.quasirandom.SobolEngine(dimension=3)\n",
    "X = sobol.draw(10000)\n",
    "f = lambda X: torch.exp(torch.sin(X[:,0]**2 + X[:,1]) + torch.cos(X[:,2]**2))\n",
    "F = f(X).reshape(-1, 1)\n",
    "X, F = X.to(device), F.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FastKAN(\n",
       "  (layers): ModuleList(\n",
       "    (0): FastKANLayer(\n",
       "      (layernorm): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "      (rbf): RadialBasisFunction()\n",
       "      (spline_linear): SplineLinear(in_features=24, out_features=2, bias=False)\n",
       "      (base_linear): Linear(in_features=3, out_features=2, bias=True)\n",
       "    )\n",
       "    (1): FastKANLayer(\n",
       "      (layernorm): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "      (rbf): RadialBasisFunction()\n",
       "      (spline_linear): SplineLinear(in_features=16, out_features=2, bias=False)\n",
       "      (base_linear): Linear(in_features=2, out_features=2, bias=True)\n",
       "    )\n",
       "    (2): FastKANLayer(\n",
       "      (layernorm): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "      (rbf): RadialBasisFunction()\n",
       "      (spline_linear): SplineLinear(in_features=16, out_features=1, bias=False)\n",
       "      (base_linear): Linear(in_features=2, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FastKAN([3, 2, 2, 1])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.LBFGS(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.7939956188201904: 100%|██████████| 10000/10000 [08:06<00:00, 20.54it/s]\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(range(10000))\n",
    "for i in pbar:\n",
    "    optimizer.zero_grad()\n",
    "    F_pred = model(X)\n",
    "    loss = ((F_pred - F)**2).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step(lambda: loss)\n",
    "    pbar.set_description(f\"loss: {loss.item()}\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
